{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RD-Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data path configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#all_discussions.csv format: \n",
    "#id_discussion;category_type;author;date;#comments;title\n",
    "\n",
    "Example:\n",
    "\n",
    "36047;help;ammund0;2022-07-02T22:08:29Z;1;[svg's with gatsby] - gatsby build process removing inline styles from svg's\n",
    "36639;help;gallardox;2022-09-19T17:42:09Z;1;how to deploy the production gatsby site.\n",
    "36608;rfc;haledgarbaya;2022-09-14T12:41:41Z;5;rfc: partial hydration\n",
    "36609;umbrella-discussions;haledgarbaya;2022-09-14T12:43:02Z;5;gatsby 5 umbrella discussion\n",
    "36635;help;amccall0;2022-09-18T23:15:24Z;0;can't deploy gatsbywp to netlify or gatsby cloud\n",
    "36063;help;rTab;2022-07-05T15:36:05Z;1;control gatsby router behaviour\n",
    "36633;help;ilyntalmers;2022-09-18T19:12:26Z;0;critical css extraction\n",
    "36631;help;aushalyap;2022-09-18T11:06:46Z;0;can we expect at least dev server performance improvement in v5?\n",
    "36626;help;tephyswe;2022-09-16T16:33:40Z;2;gatsby build service w/o restoring cache?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project_name = \"homebrew\"\n",
    "#repo = \"Homebrew/discussions/\"\n",
    "#f1 = \"all_discussions.csv\"\n",
    "\n",
    "project_name = \"gatsby\"\n",
    "repo = \"gatsbyjs/gatsby/\"\n",
    "f1 = \"all_discussions.csv\"\n",
    "\n",
    "#repoprojeto = \"vercel/next.js/\"\n",
    "#project_name = \"next_js\"\n",
    "#f1 = \"all_discussions.csv\"\n",
    "\n",
    "path_data = \"github/\"+project_name+\"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RD-Detector: preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lemmatization(sentence):\n",
    "    \n",
    "    texto = \" \".join([wordnet_lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence) if w not in string.punctuation])\n",
    "    return(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stop = \"e.g,0o,0s,3a,3b,3d,6b,6o,a,a1,a2,a3,a4,ab,able,about,above,abst,ac,ad,ae,af,ag,ah,ain,ain't,aj,al,all,am,among,amongst,amoungst,an,and,announce,another,any,ao,ap,ar,are,aren,arent,aren't,as,a's,aside,at,au,auth,av,aw,away,awfully,ax,ay,az,b,b1,b2,b3,ba,back,bc,bd,be,bi,bill,biol,bj,bk,bl,bn,both,bottom,bp,br,brief,briefly,bs,bt,bu,but,bx,by,c,c1,c2,c3,ca,call,came,can,cannot,cant,can't,cause,causes,cc,cd,ce,certainly,cf,cg,ch,ci,cit,cj,cl,clearly,cm,c'mon,cn,co,com,come,comes,con,couldn,couldnt,couldn't,cp,cq,cr,cry,cs,c's,ct,cu,currently,cv,cx,cy,cz,d,d2,da,date,dc,dd,de,df,di,did,didn,didn't,dj,dk,dl,do,doe,does,doesn,doesn't,doing,don,done,don't,down,downwards,dp,dr,ds,dt,du,due,during,dx,dy,e,e2,e3,ea,each,ec,ed,edu,ee,ef,effect,eg,ei,ej,el,eleven,else,elsewhere,em,en,end,ending,eo,ep,eq,er,es,especially,est,et,et-al,etc,eu,ev,even,ever,every,everybody,everyone,everything,everywhere,ex,ey,f,f2,fa,far,fc,few,ff,fi,fifteen,fifth,fify,fill,find,fire,first,five,fix,fj,fl,fn,fo,former,fr,from,front,fs,ft,fu,full,fy,g,ga,gave,ge,get,gets,getting,gi,give,given,gives,giving,gj,gl,go,goes,going,gone,got,gotten,gr,greetings,gs,gy,h,h2,h3,had,hadn,hadn't,happens,hardly,has,hasn,hasnt,hasn't,have,haven,haven't,having,he,hed,he'd,he'll,hello,help,hence,her,here,hereafter,hereby,herein,heres,here's,hereupon,hers,herself,hes,he's,hh,hi,hid,him,himself,his,hither,hj,ho,home,hopefully,how,howbeit,however,how's,hr,hs,http,hu,hundred,hy,i,i2,i3,i4,i6,i7,i8,ia,ib,ibid,ic,id,i'd,ie,if,ig,ignored,ih,ii,ij,il,i'll,im,i'm,in,inasmuch,inc,indeed,insofar,instead,interest,invention,inward,io,ip,iq,ir,is,isn,isn't,it,itd,it'd,it'll,its,it's,itself,iv,i've,ix,iy,iz,j,jj,jr,js,jt,ju,just,k,ke,keep,keeps,kept,kg,kj,km,know,known,knows,ko,l,l2,la,largely,last,lately,later,latter,latterly,lb,lc,le,least,les,less,lest,let,lets,let's,lf,like,liked,likely,lj,ll,ll,ln,lo,look,looking,looks,los,lr,ls,lt,ltd,m,m2,ma,made,mainly,make,makes,many,may,maybe,me,mean,means,meantime,meanwhile,merely,mg,might,mightn,mightn't,mill,million,mine,miss,ml,mn,mo,more,moreover,most,mostly,move,mr,mrs,ms,mt,mu,much,mug,must,mustn,mustn't,my,myself,n,n2,na,name,namely,nay,nc,nd,ne,near,nearly,necessarily,necessary,need,needn,needn't,needs,neither,never,nevertheless,new,next,ng,ni,nine,ninety,nj,nl,nn,no,nobody,non,none,nonetheless,noone,nor,normally,nos,not,noted,nothing,now,nowhere,nr,ns,nt,ny,o,oa,ob,oc,od,of,off,often,og,oh,oi,oj,ok,okay,ol,old,om,omitted,on,once,one,ones,only,onto,oo,op,oq,or,ord,os,ot,other,others,otherwise,ou,ought,our,ours,ourselves,out,outside,over,overall,ow,owing,own,ox,oz,p,p1,p2,p3,par,part,pas,past,pc,pd,pe,people,per,perhaps,pf,ph,pi,pj,pk,pl,please,plus,pm,pn,po,poorly,pp,pq,pr,previously,primarily,probably,promptly,proud,provides,ps,pt,pu,put,py,q,qj,qu,que,quickly,quite,qv,r,r2,ra,ran,rather,rc,rd,re,readily,really,rf,rh,ri,right,rj,rl,rm,rn,ro,rq,rr,rs,rt,ru,run,rv,ry,s,s2,sa,said,same,saw,say,saying,says,sc,sd,se,sec,second,secondly,section,see,seeing,seem,seemed,seeming,seems,seen,self,selves,sensible,sent,serious,seriously,seven,several,sf,shall,shan,shan't,she,shed,she'd,she'll,shes,she's,should,shouldn,shouldn't,should've,show,showed,shown,showns,shows,si,sj,sl,slightly,sm,sn,so,some,somebody,somehow,someone,somethan,something,sometime,sometimes,somewhat,somewhere,soon,sorry,sp,sq,sr,ss,st,still,strongly,sub,substantially,sufficiently,sup,sure,sy,system,sz,t,t1,t2,t3,tb,tc,td,te,tell,ten,tends,tf,th,than,thank,thanks,thanx,that,that'll,thats,that's,that've,the,their,theirs,them,themselves,then,thence,there,thereafter,thereby,thered,therefore,therein,there'll,thereof,therere,theres,there's,thereto,thereupon,there've,these,they,theyd,they'd,they'll,theyre,they're,they've,thickv,thin,think,third,this,thorough,thoroughly,those,thou,though,thoughh,throug,through,throughout,thru,thus,ti,til,tip,tj,tl,tm,tn,to,too,took,top,toward,towards,tp,tq,tr,truly,ts,t's,tt,tx,u,u201d,ue,ui,uj,uk,um,un,unfortunately,unless,unlike,unlikely,until,unto,uo,ur,us,ut,v,va,vd,ve,ve,very,via,viz,vj,vo,vol,vols,volumtype,vq,vs,vt,vu,w,wa,want,wants,was,wasn,wasnt,wasn't,way,we,wed,we'd,welcome,we'll,well-b,went,were,we're,weren,werent,weren't,we've,what,whatever,what'll,whats,what's,when,whence,whenever,when's,whereafter,whereas,whereby,wherein,wheres,where's,whereupon,whim,whither,who,whod,whoever,whole,who'll,whom,whomever,whos,who's,whose,why,why's,wi,widely,will,willing,wish,wo,won,wonder,wont,won't,words,world,would,wouldn,wouldnt,wouldn't,www,xf,xi,xj,xk,xl,xn,xo,xs,xt,xv,xx,y,y2,yes,yet,yj,yl,you,youd,you'd,you'll,your,youre,you're,yours,yourself,yourselves,you've,yr,ys,yt,z,zero,zi,zz\"\n",
    "l_new_stop = new_stop.split(\",\") \n",
    "\n",
    "stop_words1 = set(stopwords.words('english'))\n",
    "stop_words1.update(l_new_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword(t):\n",
    "    word_tokens = word_tokenize(t)\n",
    "    filtered_sentence = \" \"\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words1: \n",
    "            filtered_sentence += \" \" + w\n",
    "    \n",
    "    return(str(filtered_sentence))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_especific_dot(t):\n",
    "    word_tokens = word_tokenize(t)\n",
    "    filtered_sentence = \" \"\n",
    "    for w in word_tokens: \n",
    "        if \".\" in w: \n",
    "            dados = w.split(\".\")\n",
    "            if (len(dados[0])==0 or len(dados[1])==0):\n",
    "                w = w.replace(\".\",\" \")\n",
    "        filtered_sentence += \" \" + w\n",
    "\n",
    "    \n",
    "    return(str(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(sentence):\n",
    "    \n",
    "    texto = \" \".join(w for w in nltk.word_tokenize(sentence) if not w.isnumeric())\n",
    "    return(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(t):\n",
    "    return re.sub(r\"http\\S+\", \" \", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "\n",
    "    punctuations = '''!`()[]{};|:'\"\\/,<>?=@$%^&*~├ ┌ ● ’ ▲↵·“”'''\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "\n",
    "    #remove url\n",
    "    text = remove_url(text)\n",
    "    \n",
    "    text = text.replace(\"c:\\\\\",\"\")\n",
    "      \n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "       \n",
    "    #remove emoji   \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       u\"\\U0001f926-\\U0001f937\"\n",
    "                       u\"\\u200d\"\n",
    "                       u\"\\u2640-\\u2642\" \n",
    "                       \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji_pattern.sub(u'', text)\n",
    "   \n",
    "\n",
    "    #remove dots\n",
    "    no_punct = \" \"\n",
    "    for char in text:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "        else:\n",
    "            no_punct = no_punct + \" \"\n",
    "    text=no_punct\n",
    "    \n",
    "    text = remove_especific_dot(text)\n",
    "  \n",
    "      \n",
    "    #remove number   \n",
    "    text = is_number(text)\n",
    "    \n",
    "       \n",
    "    text = apply_lemmatization(text)\n",
    "    \n",
    "    #remove stopword\n",
    "    text = remove_stopword(text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_lines(path,f):\n",
    "    f = open(path+f,'r', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    return (lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(path,f):\n",
    "    l = read_file_lines(path,f)\n",
    "    text= \"\"\n",
    "    text = \" \".join(l)\n",
    "    return (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tupla(l_tuplas, f, path=path_data):\n",
    "    \n",
    "    file = open(path+f,\"a\")\n",
    "    for l in l_tuplas:\n",
    "        file.write(str(l[0])+\";\"+str(l[1])+\";\"+str(l[2])+\"\\n\")\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(path,t,name_file):\n",
    "    file = open(path+name_file,\"a\")\n",
    "    file.write(t+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing discussion posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_disc_main_post = \"clean_semComment_3\"\n",
    "\n",
    "\n",
    "#Set the catecory filter c\n",
    "category_types = ['help', 'ideas', 'ideas-feature-requests', 'umbrella-discussions', 'community', 'rfc','ALL']\n",
    "category = category_types[2]\n",
    "\n",
    "\n",
    "def load_dataset_discussions():\n",
    "    all_disc_data = []\n",
    "    \n",
    "    discussions = read_file_lines(path_data,f1)\n",
    "      \n",
    "    for lin in discussions:\n",
    "        disc_data = []\n",
    "        \n",
    "        dados = lin.split(\";\")\n",
    "        id_discussion = dados[0]\n",
    "        category_discussion = dados[1]\n",
    "        \n",
    "        if (category_discussion == category):\n",
    "        #if (True): #For category = \"ALL\"\n",
    "            disc_data.append(str(id_discussion))\n",
    "            \n",
    "            title_discussion = \" \".join(dados[5:])\n",
    "            main_discussion_text = read_file_content(path_data+\"discussions/\"+dir_disc_main_post+\"/\",id_discussion+\".txt\")\n",
    "            \n",
    "            disc_text = title_discussion + \" \" + main_discussion_text   \n",
    "            \n",
    "            text_clean = pre_process(disc_text)\n",
    "        \n",
    "            # Detecting zero-length discussion posts\n",
    "            #if (len(text_clean) == 0):\n",
    "             #print(id_discussion,\";\",main_discussion_text) \n",
    "    \n",
    "            disc_data.append(str(disc_text))\n",
    "            disc_data.append(str(text_clean))\n",
    "            all_disc_data.append(disc_data)\n",
    "                   \n",
    "        \n",
    "    return(all_disc_data)\n",
    "\n",
    "\n",
    "all_disc_data = []\n",
    "all_disc_data = load_dataset_discussions()\n",
    "print(\"#Discussion posts:\",len(all_disc_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussions_df = pd.DataFrame (all_disc_data, columns = ['ident', 'content','cleaned'])\n",
    "print (discussions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RD-Detector: Relatedness Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relatedness Checker: Similarity Checher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading sentence-Bert model:all-mpnet-base-v2\n",
    "sbert_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ceating the discussions' embeddings\n",
    "document_embeddings = sbert_model.encode(discussions_df['cleaned'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the K value\n",
    "#doc: https://www.sbert.net/examples/applications/paraphrase-mining/README.html\n",
    "#top_k – For each sentence, we retrieve up to top_k other sentences\n",
    "#score_function – Function for computing scores. By default, cosine similarity.\n",
    "\n",
    "k= 2 #k=5\n",
    "paraphrases = util.paraphrase_mining(sbert_model, discussions_df['cleaned'], corpus_chunk_size=len(discussions_df['cleaned']), top_k = k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relatedness Checker: Threshold definition (T_related)  & Selection of the related discussion candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_related_discussion_candidates(sim_values,l_inf,N):\n",
    "    tot_disc=0;\n",
    "    similares = []\n",
    "    \n",
    "    for master, target, sim1 in sim_values:\n",
    "        sim = float(sim1)\n",
    "\n",
    "        if (sim >= l_inf):\n",
    "            #selecting unique pairs of related discussion candidates\n",
    "            if (target+\";\"+master not in similares):               \n",
    "                r_candidates = master + \";\" + target\n",
    "                similares.append(r_candidates)\n",
    "                tot_disc +=1                \n",
    "                #saving the RD-Detector output\n",
    "                save_file(path_data+\"rd-detector_output/\",r_candidates,\"R_\"+project_name+\"top_\"+str(N)+\"_\"+category+\".csv\")                                         \n",
    "    return (tot_disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sim_top=[]\n",
    "all_bert_values =[]\n",
    "\n",
    "for paraphrase in paraphrases:\n",
    "    score, i, j = paraphrase  \n",
    "    #(master,target,similarity_value)\n",
    "    tupla = (discussions_df.iloc[i]['ident'],discussions_df.iloc[j]['ident'], score)\n",
    "    all_bert_values.append(tupla)\n",
    "    all_sim_top.append(score)\n",
    "\n",
    "#saving the similarity values    \n",
    "save_tupla(all_bert_values,project_name+\"_top\"+str(k)+\"_\"+category+\"_similaryValues.csv\",\"\")\n",
    "\n",
    "\n",
    "# descriptive statistic\n",
    "v_all_sim = np.array(all_sim_top)\n",
    "q75, q50, q25 = np.percentile(v_all_sim, [75 , 50, 25])\n",
    "iqr = q75 - q25 \n",
    "\n",
    "# threshold t_related == innerfence\n",
    "innerfence = q75 + (1.5*iqr)\n",
    "    \n",
    "print(\"******\")\n",
    "print(\"Top:\",k)\n",
    "print(\"Q25:\",q25)\n",
    "print(\"Median:\",q50)\n",
    "print(\"Q75:\",q75)\n",
    "print(\"IQR:\",iqr)\n",
    "print(\"Innerfence value:\",innerfence)\n",
    "\n",
    "### OUTPUT - Selection of the related discussion candidates\n",
    "select_related_discussion_candidates(all_bert_values,innerfence,k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
